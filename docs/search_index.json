[["index.html", "A Minimal Book Example Chapter 1 Prerequisites", " A Minimal Book Example Yihui Xie 2021-05-31 Chapter 1 Prerequisites This is a sample book written in Markdown. You can use anything that Pandoc’s Markdown supports, e.g., a math equation \\(a^2 + b^2 = c^2\\). The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) Remember each Rmd file contains one and only one chapter, and a chapter is defined by the first-level heading #. To compile this example to PDF, you need XeLaTeX. You are recommended to install TinyTeX (which includes XeLaTeX): https://yihui.name/tinytex/. "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 4. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package1 in this sample book, which was built on top of R Markdown and knitr.2 References "],["literature.html", "Chapter 3 Literature", " Chapter 3 Literature Here is a review of existing methods. "],["methods.html", "Chapter 4 Methods", " Chapter 4 Methods We describe our methods in this chapter. "],["applications.html", "Chapter 5 Applications 5.1 Example one 5.2 Example two", " Chapter 5 Applications Some significant applications are demonstrated in this chapter. 5.1 Example one 5.2 Example two "],["final-words.html", "Chapter 6 Final Words", " Chapter 6 Final Words We have finished a nice book. "],["references.html", "References", " References "],["regression-and-categorical-variables.html", "Chapter 7 Regression and Categorical Variables", " Chapter 7 Regression and Categorical Variables library(tidymodels) ## Registered S3 method overwritten by &#39;tune&#39;: ## method from ## required_pkgs.model_spec parsnip ## ── Attaching packages ────────────────────────────────────── tidymodels 0.1.3 ── ## ✓ broom 0.7.6 ✓ recipes 0.1.16 ## ✓ dials 0.0.9 ✓ rsample 0.1.0 ## ✓ dplyr 1.0.6 ✓ tibble 3.1.2 ## ✓ ggplot2 3.3.3 ✓ tidyr 1.1.3 ## ✓ infer 0.5.4 ✓ tune 0.1.5 ## ✓ modeldata 0.1.0 ✓ workflows 0.2.2 ## ✓ parsnip 0.1.6 ✓ workflowsets 0.0.2 ## ✓ purrr 0.3.4 ✓ yardstick 0.0.8 ## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ── ## x purrr::discard() masks scales::discard() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x recipes::step() masks stats::step() ## • Use tidymodels_prefer() to resolve common conflicts. library(ggplot2) There is a profound connection between linear regression and ANOVA. In order to see this, you have to understand that the categorical variables of an ANOVA can be coded with numbers, which allows them to be used in a linear regression model. Let us recall3 the multiple linear regression model. Given a random sample of \\(n\\) observations \\((Y_{i}, X_{i1}, . . ., X_{ip}),\\ i=1,...,n\\), the basic multiple linear regression model is \\[ Y_{i}=\\beta_0+\\beta_1X_{i1}+...+\\beta_pX_{ip}+\\epsilon_i,\\quad i=1,...,n \\] where each \\(\\epsilon_i\\) is a random variable with a mean of \\(0\\). In matrix form, this can be written as \\[ \\begin{bmatrix} Y_1\\\\ Y_2\\\\ \\vdots\\\\ Y_n \\end{bmatrix} = \\begin{bmatrix} 1 &amp; X_{1,1} &amp; X_{1,2} &amp; \\dots &amp; X_{1, p}\\\\ 1 &amp; X_{2,1} &amp; X_{2,2} &amp; \\dots &amp; X_{2, p}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; X_{n,1} &amp; X_{n,2} &amp; \\dots &amp; X_{n, p}\\\\ \\end{bmatrix} \\begin{bmatrix} \\beta_0\\\\ \\beta_1\\\\ \\vdots\\\\ \\beta_p\\\\ \\end{bmatrix} + \\begin{bmatrix} \\epsilon_0\\\\ \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_n\\\\ \\end{bmatrix} \\] Here, the \\(X_{i,j}\\) represent our coded categorical variables. These categorical variables are coded according to the hypotheses of interest. In many cases, the coding is done so that the newly coded variables are contrasts of the old categorical variables. A contrast is a linear combination of variables such that the coefficients sum to 0. \\[\\sum_i{a_i\\theta_i}\\quad\\text{such that}\\quad\\sum_i{a_i}=0\\] Unlike in ANOVA, iN regression, it is best to use coding schemes based on orthogonal and fractional contrasts. Orthogonal contrasts are a set of contrasts in which, for any distinct pair, the sum of the cross-products of the coefficients is 0. \\[\\sum_i{a_ib_i}=0\\] I believe that a fractional contrast is such that \\[\\sum_i{|a_i|}=2\\] Categorical variable coding schemes can be easily expressed in a matrix format. The convention is to have the old categorical variables as the row headers and the newly coded variables as the column headers. In such a matrix, the \\([c_{ij}]\\) entry indicates the value of the \\(j^{th}\\) level of the new variable for the \\(i^{th}\\) level of the old variable. Here is an example of such a matrix constructed using orthogonal and fractional contrasts. (contr_mat = matrix(data = c(1,0,-1,0.5,-1,0.5), nrow = 3, ncol = 2)) ## [,1] [,2] ## [1,] 1 0.5 ## [2,] 0 -1.0 ## [3,] -1 0.5 Interpreting this coding scheme in the context of our linear model, we see that \\[ \\begin{aligned} E(Y_i|X_{i1}=1,X_{i2}=\\tfrac{1}{2}) &amp;= \\beta_0+\\beta_1+\\tfrac{1}{2}\\beta_2 &amp;= \\mu_1 \\\\ E(Y_i|X_{i1}=0,X_{i2}=-1) &amp;= \\beta_0-\\beta_2 &amp;= \\mu_2\\\\ E(Y_i|X_{i1}=-1,X_{i2}=\\tfrac{1}{2}) &amp;= \\beta_0-\\beta_1+\\tfrac{1}{2}\\beta_2 &amp;= \\mu_3 \\end{aligned} \\] or, in matrix format, \\[ \\begin{bmatrix} 1 &amp; 1 &amp; \\tfrac{1}{2} \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; -1 &amp; \\tfrac{1}{2} \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix} = \\begin{bmatrix} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\end{bmatrix} \\] We can solve this for \\(\\boldsymbol{\\beta}\\) for interpretation’s sake. solve(cbind(rep(1, nrow(contr_mat)), contr_mat)) ## [,1] [,2] [,3] ## [1,] 0.3333333 0.3333333 0.3333333 ## [2,] 0.5000000 0.0000000 -0.5000000 ## [3,] 0.3333333 -0.6666667 0.3333333 \\[ \\begin{aligned} \\beta_0 &amp;= \\tfrac{\\mu_1+\\mu_2+\\mu_3}{3} = \\text{grand mean response} \\\\ 2\\beta_1 &amp;= \\mu_1 - \\mu_3 = \\text{difference in the mean response between levels 1 &amp; 3 of the old categorical variable}\\\\ \\tfrac{3}{2}\\beta_2 &amp;= \\tfrac{\\mu_1+\\mu_3}{2} - \\mu_2 = \\text{difference in the mean response between the average of levels 1 &amp; 3 and level 2 of the old categorical variable} \\end{aligned} \\] Let’s look at another contrast matrix and see if we can interpret it. contr.helmert(n = 3) ## [,1] [,2] ## 1 -1 -1 ## 2 1 -1 ## 3 0 2 solve(cbind(rep(1, 3), contr.helmert(n = 3))) ## 1 2 3 ## [1,] 0.3333333 0.3333333 0.3333333 ## [2,] -0.5000000 0.5000000 0.0000000 ## [3,] -0.1666667 -0.1666667 0.3333333 \\[ \\begin{aligned} \\beta_0 &amp;= \\tfrac{\\mu_1+\\mu_2+\\mu_3}{3} = \\text{grand mean response} \\\\ 2\\beta_1 &amp;= \\mu_2 - \\mu_1 = \\text{difference in the mean response between levels 2 &amp; 1 of the old categorical variable}\\\\ 3\\beta_2 &amp;= \\mu_3 - \\tfrac{\\mu_1+\\mu_2}{2} = \\text{difference in the mean response between level 3 and the average of levels 1 &amp; 2 of the old categorical variable} \\end{aligned} \\] Perhaps you have heard of polynomial regression? Polynomial regression is just a special case of linear regression in a different basis. In polynomial regression, (just like multiple linear regression) if you use all of your explanatory variables, then you will likely get multi-collinearity problems. contr.poly(n=3) ## .L .Q ## [1,] -7.071068e-01 0.4082483 ## [2,] -7.850462e-17 -0.8164966 ## [3,] 7.071068e-01 0.4082483 (A = solve(cbind(rep(1, 3), contr.poly(n=3)))) ## [,1] [,2] [,3] ## 0.3333333 0.3333333 0.3333333 ## .L -0.7071068 0.0000000 0.7071068 ## .Q 0.4082483 -0.8164966 0.4082483 The first matrix shows how to code the levels of your categorical variable and the second matrix is used for interpretation. \\[ \\begin{aligned} \\beta_0 &amp;= \\tfrac{\\mu_1+\\mu_2+\\mu_3}{3} = \\text{grand mean response} \\\\ \\beta_1 &amp;= -0.707\\mu_1 + 0.707\\mu_3 = \\text{measure of a linear trend in the mean response}\\\\ \\beta_2 &amp;= 0.408 \\mu_3 - 0.816 \\mu_2 + 0.408 \\mu3 = \\text{measure of a quadratic trend in the mean response} \\end{aligned} \\] For example, we can test whether the difference between the means from two populations are equal by doing a linear regression or an ANOVA. Let’s make up some data and try it! source(file.path(&quot;R-source-code&quot;, &quot;fabricate.R&quot;)) design = data.frame(group = c(0, 1), n = c(10, 10)) data1 = fabricate(flr = design) Let’s check out our data. # Make a linear model data1_lm_independent_samples = lm(response ~ group, data = data1) # plot ggplot(data = data1, aes(x = group, y = response, color = factor(group))) + geom_boxplot() + geom_jitter(height = 0, width = 0.1) + geom_abline(intercept = data1_lm_independent_samples$coefficients[1], slope = data1_lm_independent_samples$coefficients[2]) + labs(title = &quot;Group Comparison from a Regression Standpoint&quot;, color = &quot;Group&quot;, x = &quot;Group&quot;, y = &quot;Response&quot;) + scale_x_discrete(limits = c(0, 1)) ## Warning: Continuous limits supplied to discrete scale. ## Did you mean `limits = factor(...)` or `scale_*_continuous()`? The way you code your categorical variables in a linear model is extremely important. Different codings lead to different interpretations of the parameters (betas) in your model. For us, our model is \\[ Y_i = \\beta_0+\\beta_{i1}X_{i1}+\\epsilon_i \\] From this, we have \\[ \\begin{aligned} E(Y_i|X_{i1}=0) &amp;=\\beta_0 \\\\ E(Y_i|X_{i1}=1) &amp;=\\beta_0 + \\beta_1 \\end{aligned} \\] From which we can derive, \\[ \\beta_1 = E(Y_i|X_{i1}=1) - E(Y_i|X_{i1}=0) \\] So, our slope estimate is the estimated amount by which the mean of group1 is above that of the mean of group0. Run linear regression summary(data1_lm_independent_samples) ## ## Call: ## lm(formula = response ~ group, data = data1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.6290 -0.6375 0.6560 1.2980 3.3130 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67.5990 0.8391 80.564 &lt;2e-16 *** ## group 0.6180 1.1866 0.521 0.609 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.653 on 18 degrees of freedom ## Multiple R-squared: 0.01485, Adjusted R-squared: -0.03989 ## F-statistic: 0.2712 on 1 and 18 DF, p-value: 0.6089 Run ANOVA data1$group = as.factor(data1$group) data1_ANOVA_independent_samples = aov(response ~ group, data = data1) summary(data1_ANOVA_independent_samples) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## group 1 1.91 1.91 0.271 0.609 ## Residuals 18 126.73 7.04 Run t-Test (data1_t_test_independent_samples = t.test(x = data1[data1$group == 1, &quot;response&quot;], y = data1[data1$group == 0, &quot;response&quot;], paired = FALSE, var.equal = TRUE)) ## ## Two Sample t-test ## ## data: data1[data1$group == 1, &quot;response&quot;] and data1[data1$group == 0, &quot;response&quot;] ## t = 0.52081, df = 18, p-value = 0.6089 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.875003 3.111003 ## sample estimates: ## mean of x mean of y ## 68.217 67.599 Notice the similarities. # Confidence interval for the difference in the means confint(data1_lm_independent_samples, &quot;group&quot;, level = 0.95) ## 2.5 % 97.5 % ## group -1.875003 3.111003 data1_t_test_independent_samples$conf.int ## [1] -1.875003 3.111003 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 # p-values with(summary(data1_lm_independent_samples), unname(pf(fstatistic[1],fstatistic[2],fstatistic[3],lower.tail=F))) ## [1] 0.60885 summary(data1_ANOVA_independent_samples)[[1]][[1, 5]] ## [1] 0.60885 data1_t_test_independent_samples$p.value ## [1] 0.60885 Now, let’s look at something else. The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli. data(&quot;CO2&quot;) CO2[sample(nrow(CO2), size = 5), ] ## Plant Type Treatment conc uptake ## 71 Mc2 Mississippi chilled 95 7.7 ## 24 Qc1 Quebec chilled 250 30.3 ## 52 Mn2 Mississippi nonchilled 250 30.6 ## 64 Mc1 Mississippi chilled 95 10.5 ## 34 Qc2 Quebec chilled 675 37.5 What is a linear model? In the context of linear regression, a linear model is a relationship between the responses and the explanatory variables that is linear in the parameters. CO2_recipe = recipe(uptake ~ ., data = CO2) %&gt;% step_dummy(c(&quot;Type&quot;, &quot;Treatment&quot;)) # see contrasts() function CO2_linear_model = linear_reg() %&gt;% set_engine(&quot;lm&quot;, contrasts = list(Plant = &quot;contr.poly&quot;)) CO2_workflow = workflow() %&gt;% add_model(CO2_linear_model) %&gt;% add_recipe(CO2_recipe) CO2_fit = CO2_workflow %&gt;% fit(data = CO2) CO2_fit %&gt;% pull_workflow_fit() %&gt;% tidy() ## # A tibble: 15 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 19.5 1.17 16.7 2.96e-26 ## 2 Plant.L -22.9 2.27 -10.1 2.17e-15 ## 3 Plant.Q -4.62 2.27 -2.03 4.57e- 2 ## 4 Plant.C 4.67 2.27 2.06 4.34e- 2 ## 5 Plant^4 2.34 2.27 1.03 3.06e- 1 ## 6 Plant^5 4.31 2.27 1.90 6.13e- 2 ## 7 Plant^6 -0.0390 2.27 -0.0172 9.86e- 1 ## 8 Plant^7 -2.04 2.27 -0.897 3.73e- 1 ## 9 Plant^8 -3.28 2.27 -1.44 1.53e- 1 ## 10 Plant^9 -9.07 2.27 -4.00 1.56e- 4 ## 11 Plant^10 0.546 2.27 0.241 8.10e- 1 ## 12 Plant^11 1.91 2.27 0.843 4.02e- 1 ## 13 conc 0.0177 0.00223 7.96 1.97e-11 ## 14 Type_Mississippi NA NA NA NA ## 15 Treatment_chilled NA NA NA NA References "],["anova-tutorial.html", "Chapter 8 ANOVA Tutorial 8.1 Step 1: Make up Data 8.2 Checking the Assumptions", " Chapter 8 ANOVA Tutorial library(cellWise) 8.1 Step 1: Make up Data # dataset1 8.2 Checking the Assumptions After running your ANOVA, check that the assumptions about the errors are met so that you can do statistical inference. Those assumptions are: \\(\\text{E}(\\epsilon_{ij})=0,\\ \\text{Var}(\\epsilon_{ij})=\\sigma_{i}^2 &lt; \\infty,\\ \\text{for all }i, j.\\) The \\(\\epsilon_{ij}\\) are mutually independent and normally distributed. \\(\\sigma_{i}^2=\\sigma^2\\ \\text{for all } i.\\) 8.2.1 Checking Assumption 1 8.2.2 Assumption 1 was violated. 8.2.3 Checking Assumption 2 8.2.4 Assumption 2 was violated. 8.2.5 Checking Assumption 3 8.2.6 Assumption 3 was violated. A variance-stabilizing transformation of the response variable may help. data(&quot;data_mortality&quot;) transformed_response = transfo(data_mortality, prestandardize = FALSE) ## ## The input data has 198 rows and 91 columns. hist(data_mortality[, 1]) hist(transformed_response$Xt[, 1]) shapiro.test(data_mortality[, 1]) ## ## Shapiro-Wilk normality test ## ## data: data_mortality[, 1] ## W = 0.86877, p-value = 4.552e-12 shapiro.test(transformed_response$Xt[, 1]) ## ## Shapiro-Wilk normality test ## ## data: transformed_response$Xt[, 1] ## W = 0.88041, p-value = 1.968e-11 "]]
